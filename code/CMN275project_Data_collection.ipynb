{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874831f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "     conda install conda=23.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d794c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e57f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='fAbv7Pqad2Ag1KmOqjh9yw', client_secret='4E72T9v7crYbo5Kau8OoJy8H_GRDag', user_agent='CMN_Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "subreddits = [\n",
    "    'gradschool', \n",
    "    'phd', \n",
    "    'AskAcademia', \n",
    "    'gradadmissions',\n",
    "    'masters', \n",
    "    'ucdavis', \n",
    "    'GradSchoolPurgatory',\n",
    "    'GradSchoolLPT',\n",
    "    'GradSchoolClassifieds',\n",
    "    'GraduateStudents'\n",
    "]\n",
    "# List of mental health keywords\n",
    "mental_health_keywords = ['stress', 'anxiety', 'depression', 'mental health', 'well-being', 'emotional', 'counseling',\n",
    "    'sad', 'upset', 'irritated', 'overwhelmed', 'burnout', 'pressure',\n",
    "    'panic', 'tension', 'nervous', 'fatigue', 'exhaustion', 'restless', 'worried', 'insomnia',\n",
    "    'hopeless', 'helpless',  'despair', 'trauma', 'phobia']\n",
    "\n",
    "\n",
    "\n",
    "# Function to check if a post contains mental health keywords and print relevant sentences\n",
    "def print_mental_health_sentences(post, output_file):\n",
    "    title_lower = post.title.lower()\n",
    "    body_lower = post.selftext.lower()\n",
    "\n",
    "    # Check if the title contains mental health keywords\n",
    "    if any(keyword in title_lower for keyword in mental_health_keywords):\n",
    "        print(f\"Title: {post.title}\", file=output_file)\n",
    "\n",
    "    # Check if the body contains mental health keywords\n",
    "    if any(keyword in body_lower for keyword in mental_health_keywords):\n",
    "        print(f\"Post Body:\", file=output_file)\n",
    "        print(post.selftext, file=output_file)\n",
    "\n",
    "    print('-'*40, file=output_file)\n",
    "\n",
    "# Open a file for writing\n",
    "with open('New_output.txt', 'w', encoding='utf-8') as output_file:\n",
    "\n",
    "    # Loop through each subreddit\n",
    "    for subreddit_name in subreddits:\n",
    "        print(f\"\\nFetching posts from r/{subreddit_name}\\n{'='*40}\", file=output_file)\n",
    "\n",
    "        # Access the subreddit and retrieve posts\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts = subreddit.hot(limit=100000)  # Adjust the limit as needed\n",
    "\n",
    "        # Print the titles and post bodies of posts related to mental health\n",
    "        for post in posts:\n",
    "            print_mental_health_sentences(post, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-printable characters\n",
    "    printable_characters = set(string.printable)\n",
    "    cleaned_text = ''.join(char for char in text if char in printable_characters)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Open the original output file for reading\n",
    "with open('New_output.txt', 'r', encoding='utf-8') as original_file:\n",
    "    # Read the content\n",
    "    original_content = original_file.read()\n",
    "\n",
    "# Apply the cleaning function to the content\n",
    "cleaned_content = clean_text(original_content)\n",
    "\n",
    "# Open the output file for writing cleaned content\n",
    "with open('cleaned_new_output.txt', 'w', encoding='utf-8') as cleaned_file:\n",
    "    cleaned_file.write(cleaned_content)\n",
    "\n",
    "print(\"Cleaning complete. Cleaned content saved to cleaned_output.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Read the cleaned content\n",
    "with open('cleaned_new_output.txt', 'r', encoding='utf-8') as cleaned_file:\n",
    "    cleaned_content = cleaned_file.read()\n",
    "\n",
    "# Analyze sentiment using TextBlob\n",
    "blob = TextBlob(cleaned_content)\n",
    "sentiment_score = blob.sentiment.polarity\n",
    "\n",
    "# Print sentiment score\n",
    "print(f\"Sentiment Score: {sentiment_score}\")\n",
    "\n",
    "# Print sentiment category\n",
    "if sentiment_score > 0:\n",
    "    print(\"Sentiment: Positive\")\n",
    "elif sentiment_score < 0:\n",
    "    print(\"Sentiment: Negative\")\n",
    "else:\n",
    "    print(\"Sentiment: Neutral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Read the cleaned content\n",
    "with open('cleaned_new_output.txt', 'r', encoding='utf-8') as cleaned_file:\n",
    "    cleaned_content = cleaned_file.read()\n",
    "\n",
    "# Analyze sentiment using TextBlob\n",
    "blob = TextBlob(cleaned_content)\n",
    "sentiment_score = blob.sentiment.polarity\n",
    "\n",
    "# Print sentiment score\n",
    "print(f\"Sentiment Score: {sentiment_score}\")\n",
    "\n",
    "# Print sentiment category\n",
    "if sentiment_score > 0:\n",
    "    print(\"Sentiment: Positive\")\n",
    "elif sentiment_score < 0:\n",
    "    print(\"Sentiment: Negative\")\n",
    "else:\n",
    "    print(\"Sentiment: Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer, MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text (replace this with your Reddit data)\n",
    "reddit_text = \"cleaned_new_output.txt\"\n",
    "# Word Tokenization\n",
    "words = word_tokenize(reddit_text.lower())  # Convert to lowercase for consistency\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n",
    "\n",
    "print(\"Word Tokens:\")\n",
    "print(filtered_words)\n",
    "\n",
    "# Phrase Tokenization\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(reddit_text.lower())\n",
    "mwe_tokenizer = MWETokenizer()\n",
    "phrases = mwe_tokenizer.tokenize(words)\n",
    "\n",
    "print(\"\\nPhrase Tokens:\")\n",
    "print(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a72bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')# Read the cleaned content from the 'cleaned_output.txt' file\n",
    "with open('cleaned_new_output.txt', 'r', encoding='utf-8') as cleaned_file:\n",
    "    cleaned_text = cleaned_file.read()\n",
    "\n",
    "# List of words related to mental health and programs\n",
    "stress_related_words = [\n",
    "    'stress', 'anxiety', 'depression', 'mental health', 'well-being', 'emotional', 'counseling',\n",
    "    'sad', 'upset', 'irritated', 'frustrated', 'lonely', 'overwhelmed', 'burnout'\n",
    "]\n",
    "\n",
    "# List of program-related words\n",
    "program_related_words = [\n",
    "    'program', 'coursework', 'doctoral', 'master\\'s', 'Ph.D.', 'MBA', 'research', 'lab-based',\n",
    "    'teaching', 'STEM', 'health', 'humanities', 'social sciences', 'interdisciplinary', 'MD', 'JD',\n",
    "    'engineering', 'business administration', 'nursing', 'education'\n",
    "]\n",
    "\n",
    "# Combine both lists\n",
    "target_words = stress_related_words + program_related_words\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(cleaned_text.lower())  # Convert to lowercase for consistency\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n",
    "\n",
    "# Count the frequency of each target word\n",
    "word_frequency = Counter(word for word in filtered_words if word in target_words)\n",
    "\n",
    "# Get the most common target words\n",
    "most_common_words = word_frequency.most_common()\n",
    "\n",
    "# Print the results\n",
    "print(\"Word Frequencies:\")\n",
    "for word, frequency in most_common_words:\n",
    "    print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c125759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the cleaned content from the 'cleaned_output.txt' file\n",
    "with open('cleaned_new_output.txt', 'r', encoding='utf-8') as cleaned_file:\n",
    "    cleaned_text = cleaned_file.read()\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "# Analyze sentiment for each sentence\n",
    "# (You can add the sentiment analysis code here)\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the cleaned content from the 'cleaned_output.txt' file\n",
    "with open('cleaned_output.txt', 'r', encoding='utf-8') as cleaned_file:\n",
    "    cleaned_text = cleaned_file.read()\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "# Analyze sentiment for each sentence\n",
    "sentiment_scores = []\n",
    "for sentence in sentences:\n",
    "    blob = TextBlob(sentence)\n",
    "    sentiment_scores.append(blob.sentiment.polarity)\n",
    "\n",
    "# Calculate the overall sentiment score\n",
    "overall_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "\n",
    "# Determine sentiment label\n",
    "if overall_sentiment > 0:\n",
    "    sentiment_label = 'Positive'\n",
    "elif overall_sentiment < 0:\n",
    "    sentiment_label = 'Negative'\n",
    "else:\n",
    "    sentiment_label = 'Neutral'\n",
    "\n",
    "# Print the results\n",
    "print(f\"Overall Sentiment: {overall_sentiment:.2f} (Label: {sentiment_label})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf528db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read text from the cleaned_output.txt file\n",
    "with open('cleaned_new_output.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud using Matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525a8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
