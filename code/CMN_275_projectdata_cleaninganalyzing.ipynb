{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_new_output.txt', 'r') as file:\n",
    "    content = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer models\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_new_output.txt', 'w') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download Vader lexicon (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Read the tokenized sentences from the file\n",
    "with open('cleaned_new_output.txt', 'r') as file:\n",
    "    tokenized_sentences = file.readlines()\n",
    "\n",
    "# Initialize the Sentiment Intensity Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis on each sentence\n",
    "sentiment_scores = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    sentiment_score = sid.polarity_scores(sentence)\n",
    "    sentiment_scores.append(sentiment_score)\n",
    "\n",
    "# Display sentiment scores\n",
    "for i, score in enumerate(sentiment_scores):\n",
    "    print(f\"Sentence {i + 1}: {score}\")\n",
    "\n",
    "# If you want a summary sentiment for the entire text\n",
    "overall_sentiment = sid.polarity_scores(' '.join(tokenized_sentences))\n",
    "print(\"\\nOverall Sentiment:\", overall_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the tokenized sentences from the file\n",
    "with open('cleaned_new_output.txt', 'r') as file:\n",
    "    tokenized_sentences = file.readlines()\n",
    "\n",
    "# Tokenize words from sentences\n",
    "all_words = [word.lower() for sentence in tokenized_sentences for word in word_tokenize(sentence)]\n",
    "\n",
    "# Define a list of stress-related, depression-related, and other relevant words\n",
    "all_related_words = [\n",
    "    'stress', 'anxiety', 'pressure', 'tension', 'strain', 'worry', 'overwhelmed',\n",
    "    'depression', 'hopelessness', 'sadness', 'despair', 'low', 'down',\n",
    "    'fatigue', 'burnout', 'exhaustion', 'nervous', 'irritable', 'lonely'\n",
    "]\n",
    "\n",
    "# Filter words that are related to stress, depression, or others\n",
    "related_words = [word for word in all_words if word in all_related_words]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = FreqDist(related_words)\n",
    "\n",
    "# Display related words and their frequencies\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Display related words and their frequencies\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq} times\")\n",
    "\n",
    "# Plotting the frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(word_freq.keys(), word_freq.values())\n",
    "plt.title('Frequency of Stress-Related, Depression-Related, and Other Relevant Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the tokenized sentences from the file\n",
    "with open('tokenized_stress_sentences.txt', 'r') as file:\n",
    "    tokenized_sentences = file.readlines()\n",
    "\n",
    "# Tokenize words from sentences\n",
    "all_words = [word.lower() for sentence in tokenized_sentences for word in word_tokenize(sentence)]\n",
    "\n",
    "# Define a list of stress-related, depression-related, and other relevant words\n",
    "all_related_words = [\n",
    "    'stress', 'anxiety', 'pressure', 'tension', 'strain', 'worry', 'overwhelmed',\n",
    "    'depression']\n",
    "\n",
    "# Filter words that are related to stress, depression, or others\n",
    "related_words = [word for word in all_words if word in all_related_words]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = FreqDist(related_words)\n",
    "\n",
    "# Display related words and their frequencies\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq} times\")\n",
    "\n",
    "# Find common words among all sentences that are also in the all_related_words list\n",
    "common_words = set(all_words).intersection(set(all_related_words))\n",
    "\n",
    "# Display common words\n",
    "print(\"\\nCommon words among all sentences and in the all_related_words list:\")\n",
    "print(common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams, word_tokenize  # Add word_tokenize import\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the tokenized sentences from the file\n",
    "with open('cleaned_new_output.txt', 'r') as file:\n",
    "    tokenized_sentences = file.readlines()\n",
    "\n",
    "# Tokenize words from sentences\n",
    "all_words = [word.lower() for sentence in tokenized_sentences for word in word_tokenize(sentence)]\n",
    "\n",
    "# Create bigrams (pairs of consecutive words)\n",
    "bi_grams = list(bigrams(all_words))\n",
    "\n",
    "# Create a co-occurrence matrix\n",
    "co_occurrence_matrix = nltk.FreqDist(bi_grams)\n",
    "\n",
    "# Create a weighted graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges and weights to the graph\n",
    "for bigram, weight in co_occurrence_matrix.items():\n",
    "    G.add_edge(bigram[0], bigram[1], weight=weight)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G, seed=42)  # Set a seed for reproducibility\n",
    "nx.draw(G, pos, with_labels=True, font_size=8, font_color='black', node_size=10, node_color='skyblue', font_weight='bold', edge_color='gray', linewidths=0.5)\n",
    "plt.title('Co-occurrence Network of Words')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the tokenized sentences from the file\n",
    "with open('cleaned_new_output.txt', 'r') as file:\n",
    "    tokenized_sentences = file.readlines()\n",
    "\n",
    "\n",
    "# Create bigrams (pairs of consecutive words)\n",
    "bi_grams = list(bigrams(all_words))\n",
    "\n",
    "# Create a co-occurrence matrix\n",
    "co_occurrence_matrix = nltk.FreqDist(bi_grams)\n",
    "\n",
    "# Prepare data for Gephi\n",
    "gephi_data = []\n",
    "for bigram, weight in co_occurrence_matrix.items():\n",
    "    gephi_data.append([bigram[0], bigram[1], weight])\n",
    "\n",
    "# Save data to CSV file\n",
    "csv_file_path = 'gephi_data.csv'\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Source', 'Target', 'Weight'])  # CSV header\n",
    "    csv_writer.writerows(gephi_data)\n",
    "\n",
    "print(f'Data has been saved to {csv_file_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0cfdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read the tokenized sentences from the file\n",
    "with open('cleaned_new_output.txt', 'r') as file:\n",
    "    tokenized_sentences = file.readlines()\n",
    "\n",
    "# Tokenize words from sentences\n",
    "all_words = [word.lower() for sentence in tokenized_sentences for word in word_tokenize(sentence)]\n",
    "\n",
    "# Define a list of stress-related words\n",
    "stress_related_words = [\n",
    "    'stress', 'anxiety', 'pressure', 'tension', 'strain', 'worry', 'overwhelmed',\n",
    "    'depression', 'hopelessness', 'sadness', 'despair', 'low', 'down',\n",
    "    'fatigue', 'burnout', 'exhaustion', 'nervous', 'irritable', 'lonely'\n",
    "]\n",
    "\n",
    "# Filter words that are related to stress\n",
    "stress_words = [word for word in all_words if word in stress_related_words]\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for stress-related words\n",
    "G.add_nodes_from(stress_words)\n",
    "\n",
    "# Add edges between co-occurring stress-related words\n",
    "for sentence in tokenized_sentences:\n",
    "    words_in_sentence = [word.lower() for word in word_tokenize(sentence)]\n",
    "    stress_words_in_sentence = [word for word in words_in_sentence if word in stress_words]\n",
    "    if len(stress_words_in_sentence) > 1:\n",
    "        for pair in itertools.combinations(stress_words_in_sentence, 2):\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "\n",
    "# Save the graph to a Gephi-compatible file\n",
    "gephi_file_path = 'gephi_data.gexf'\n",
    "nx.write_gexf(G, gephi_file_path)\n",
    "\n",
    "print(f'Data has been saved to {gephi_file_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d188816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read your tokenized stress sentences file\n",
    "with open('cleaned_new_output.txt', 'r', encoding='utf-8') as file:\n",
    "    stress_sentences = [line.strip().split() for line in file.readlines()]\n",
    "\n",
    "# Stress-related words\n",
    "stress_related_words = [\n",
    "    'stress', 'anxiety', 'depression', 'mental health', 'well-being', 'emotional', 'counseling',\n",
    "    'sad', 'upset', 'irritated', 'overwhelmed', 'burnout', 'pressure',\n",
    "    'tension', 'nervous', 'fatigue', 'exhaustion', 'restless', 'worried', 'insomnia',\n",
    "    'hopeless', 'helpless', 'trauma', 'phobia'\n",
    "]\n",
    "\n",
    "# Program-related words\n",
    "program_related_words = [\n",
    "    'program', 'coursework', 'degree', 'doctoral', 'master\\'s', 'Ph.D.', 'MBA', 'research', 'lab-based',\n",
    "    'STEM', 'health', 'doctor', 'tuition', 'fees',\n",
    "    'engineering', 'business administration', 'education', 'study', 'academic',\n",
    "    'project', 'assignment', 'lecture', 'thesis',\n",
    "    'scholarship'\n",
    "]\n",
    "\n",
    "# Remove potentially less relevant words\n",
    "# You can customize this based on your specific needs\n",
    "stress_related_words = [word for word in stress_related_words if word not in ['pressure', 'panic', 'tension', 'nervous']]\n",
    "program_related_words = [word for word in program_related_words if word not in ['study', 'learning', 'academic']]\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for stress-related and program-related words\n",
    "G.add_nodes_from(stress_related_words, category='stress')\n",
    "G.add_nodes_from(program_related_words, category='program')\n",
    "\n",
    "# Iterate through stress sentences and add edges between stress and program-related words\n",
    "for sentence in stress_sentences:\n",
    "    stress_words_in_sentence = [word for word in sentence if word in stress_related_words]\n",
    "    program_words_in_sentence = [word for word in sentence if word in program_related_words]\n",
    "\n",
    "    for stress_word in stress_words_in_sentence:\n",
    "        for program_word in program_words_in_sentence:\n",
    "            # Add an edge between stress and program-related words in the same sentence\n",
    "            G.add_edge(stress_word, program_word)\n",
    "\n",
    "# Visualize the graph with improved layout and style\n",
    "pos = nx.spring_layout(G, seed=2000, k=100)  # Increase spacing between nodes\n",
    "\n",
    "# Create a dictionary of node colors based on their categories\n",
    "node_colors = {node: 'pink' if G.nodes[node]['category'] == 'stress' else 'lightblue' for node in G.nodes}\n",
    "\n",
    "# Draw the graph with improved style\n",
    "nx.draw(G, pos, with_labels=True, font_size=7, font_color='black', node_size=300,\n",
    "        node_color=[node_colors[node] for node in G.nodes()], font_weight='bold', edge_color='red', linewidths=1.0)  # Increase edge thickness\n",
    "plt.title('Co-occurrence Network of Stress and Program-related Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a090c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (previous code remains unchanged)\n",
    "\n",
    "# Visualize the graph with improved layout and style\n",
    "pos = nx.kamada_kawai_layout(G)  # Use a different layout algorithm\n",
    "node_colors = {'stress': 'salmon', 'program': 'lightblue'}\n",
    "\n",
    "# Adjust node size\n",
    "node_size = 1000\n",
    "\n",
    "# Filter out edges with weight below a certain threshold\n",
    "threshold = 2\n",
    "filtered_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('weight', 1) > threshold]\n",
    "\n",
    "# Draw nodes and edges separately to customize their appearance\n",
    "nx.draw_networkx_nodes(G, pos, node_color=[node_colors[G.nodes[node]['category']] for node in G.nodes], node_size=node_size)\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_color='black', font_weight='bold', verticalalignment=\"center\")  # Adjust label positions\n",
    "nx.draw_networkx_edges(G, pos, edgelist=filtered_edges, edge_color='gray', width=2)  # Use thicker lines for edges\n",
    "\n",
    "plt.title('Co-occurrence Network of Stress and Program-related Words')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install monkeylearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monkeylearn import MonkeyLearn\n",
    "\n",
    "# Your MonkeyLearn API key\n",
    "api_key = 'your_api_key'\n",
    "monkeylearn = MonkeyLearn(api_key)\n",
    "\n",
    "# Define the model ID for emotion analysis\n",
    "model_id = 'cl_pi3C7JiL'\n",
    "\n",
    "# Read content from your text file\n",
    "file_path = 'cleaned_new_output.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    script_text = file.read()\n",
    "\n",
    "# Make a request to MonkeyLearn for emotion analysis\n",
    "result = monkeylearn.classifiers.classify(model_id, [script_text])\n",
    "\n",
    "# Extract the predicted emotion\n",
    "predicted_emotion = result.body[0]['classifications'][0]['tag_name']\n",
    "\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb4115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
